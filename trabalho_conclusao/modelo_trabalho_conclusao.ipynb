{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo - Validação - Teste 📊 🧪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pl.read_parquet(\"data/transformed_train_data/sessions_with_more_than_2_clicks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10161584"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>session</th><th>sorted_events</th><th>items_clicked</th><th>items_carted</th><th>items_ordered</th><th>items_clicked_count</th><th>items_carted_count</th><th>items_ordered_count</th></tr><tr><td>i64</td><td>list[struct[3]]</td><td>list[i64]</td><td>list[i64]</td><td>list[i64]</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>12125765</td><td>[{236629,1661455845476,&quot;clicks&quot;}, {510059,1661455883265,&quot;clicks&quot;}, {236629,1661455939286,&quot;clicks&quot;}]</td><td>[236629, 510059, 236629]</td><td>[]</td><td>[]</td><td>3</td><td>0</td><td>0</td></tr><tr><td>12125766</td><td>[{1119434,1661455845769,&quot;clicks&quot;}, {1119434,1661455891538,&quot;clicks&quot;}, … {1119434,1661498592931,&quot;clicks&quot;}]</td><td>[1119434, 1119434, … 1119434]</td><td>[]</td><td>[]</td><td>18</td><td>0</td><td>0</td></tr><tr><td>12125767</td><td>[{1321238,1661455845771,&quot;clicks&quot;}, {1389738,1661455955373,&quot;clicks&quot;}, … {129869,1661456097095,&quot;clicks&quot;}]</td><td>[1321238, 1389738, … 129869]</td><td>[]</td><td>[]</td><td>6</td><td>0</td><td>0</td></tr><tr><td>12125768</td><td>[{1676761,1661455845774,&quot;clicks&quot;}, {1676761,1661455899065,&quot;clicks&quot;}, … {661746,1661717837490,&quot;clicks&quot;}]</td><td>[1676761, 1676761, … 661746]</td><td>[]</td><td>[]</td><td>9</td><td>0</td><td>0</td></tr><tr><td>12125770</td><td>[{917900,1661455846705,&quot;clicks&quot;}, {1033924,1661714173716,&quot;clicks&quot;}, … {1033924,1661714269437,&quot;clicks&quot;}]</td><td>[917900, 1033924, … 1033924]</td><td>[]</td><td>[]</td><td>4</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "┌──────────┬────────────┬────────────┬────────────┬────────────┬───────────┬───────────┬───────────┐\n",
       "│ session  ┆ sorted_eve ┆ items_clic ┆ items_cart ┆ items_orde ┆ items_cli ┆ items_car ┆ items_ord │\n",
       "│ ---      ┆ nts        ┆ ked        ┆ ed         ┆ red        ┆ cked_coun ┆ ted_count ┆ ered_coun │\n",
       "│ i64      ┆ ---        ┆ ---        ┆ ---        ┆ ---        ┆ t         ┆ ---       ┆ t         │\n",
       "│          ┆ list[struc ┆ list[i64]  ┆ list[i64]  ┆ list[i64]  ┆ ---       ┆ i64       ┆ ---       │\n",
       "│          ┆ t[3]]      ┆            ┆            ┆            ┆ i64       ┆           ┆ i64       │\n",
       "╞══════════╪════════════╪════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 12125765 ┆ [{236629,1 ┆ [236629,   ┆ []         ┆ []         ┆ 3         ┆ 0         ┆ 0         │\n",
       "│          ┆ 6614558454 ┆ 510059,    ┆            ┆            ┆           ┆           ┆           │\n",
       "│          ┆ 76,\"clicks ┆ 236629]    ┆            ┆            ┆           ┆           ┆           │\n",
       "│          ┆ \"}…        ┆            ┆            ┆            ┆           ┆           ┆           │\n",
       "│ 12125766 ┆ [{1119434, ┆ [1119434,  ┆ []         ┆ []         ┆ 18        ┆ 0         ┆ 0         │\n",
       "│          ┆ 1661455845 ┆ 1119434, … ┆            ┆            ┆           ┆           ┆           │\n",
       "│          ┆ 769,\"click ┆ 1119434]   ┆            ┆            ┆           ┆           ┆           │\n",
       "│          ┆ s\"…        ┆            ┆            ┆            ┆           ┆           ┆           │\n",
       "│ 12125767 ┆ [{1321238, ┆ [1321238,  ┆ []         ┆ []         ┆ 6         ┆ 0         ┆ 0         │\n",
       "│          ┆ 1661455845 ┆ 1389738, … ┆            ┆            ┆           ┆           ┆           │\n",
       "│          ┆ 771,\"click ┆ 129869]    ┆            ┆            ┆           ┆           ┆           │\n",
       "│          ┆ s\"…        ┆            ┆            ┆            ┆           ┆           ┆           │\n",
       "│ 12125768 ┆ [{1676761, ┆ [1676761,  ┆ []         ┆ []         ┆ 9         ┆ 0         ┆ 0         │\n",
       "│          ┆ 1661455845 ┆ 1676761, … ┆            ┆            ┆           ┆           ┆           │\n",
       "│          ┆ 774,\"click ┆ 661746]    ┆            ┆            ┆           ┆           ┆           │\n",
       "│          ┆ s\"…        ┆            ┆            ┆            ┆           ┆           ┆           │\n",
       "│ 12125770 ┆ [{917900,1 ┆ [917900,   ┆ []         ┆ []         ┆ 4         ┆ 0         ┆ 0         │\n",
       "│          ┆ 6614558467 ┆ 1033924, … ┆            ┆            ┆           ┆           ┆           │\n",
       "│          ┆ 05,\"clicks ┆ 1033924]   ┆            ┆            ┆           ┆           ┆           │\n",
       "│          ┆ \"}…        ┆            ┆            ┆            ┆           ┆           ┆           │\n",
       "└──────────┴────────────┴────────────┴────────────┴────────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_dataset.shape[0])\n",
    "display(train_dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10161584/10161584 [00:04<00:00, 2303675.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# count all possible items clicked\n",
    "all_items = []\n",
    "for session_items in tqdm.tqdm(train_dataset[\"items_clicked\"].to_list()):\n",
    "    all_items.extend(session_items)\n",
    "    \n",
    "all_items = list(set(all_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1855574"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(all_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando dados de treinamento 🏋️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def generate_training_data(\n",
    "    dataset: pl.DataFrame,\n",
    "    window_size: int = 2,\n",
    "    num_negatives: int = 3,\n",
    "    num_products: int = 1855574,\n",
    "    seed: int = 42,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Gera os dados de treinamento para o modelo Prod2Vec.\n",
    "    Tipo: pares de skip-gram\n",
    "    \n",
    "    Args:\n",
    "    ------\n",
    "        * dataset (pl.DataFrame): Dataset containing the sessions with more than 2 clicks(must have items_clicked column!).\n",
    "        * window_size (int): Size of the window to be used in the skip-gram model.\n",
    "        * num_negatives (int): Number of negative samples to be used in the loss function.\n",
    "        * num_products (int): Number of products in the dataset.\n",
    "        * seed (int): Seed to be used in the random number generator.\n",
    "    \"\"\"\n",
    "    targets, contexts, labels = [], [], []\n",
    "    \n",
    "    # Build sampling table for `num_products` products assuming Zipf's law distribution for the frequencies.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(num_products)\n",
    "    \n",
    "   \n",
    "    # iterate through all sequences (sessions) in the dataset \n",
    "    for sequence in tqdm.tqdm(dataset[\"items_clicked\"].to_list()):\n",
    "        # Generate positive skip-gram pairs for a sequence (session).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequence,\n",
    "            vocabulary_size=num_products,\n",
    "            window_size=window_size,\n",
    "            sampling_table=sampling_table,\n",
    "            negative_samples=0,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "        )\n",
    "        \n",
    "        # Iterate over each positive skip-gram pair to produce training examples\n",
    "        # with positive context product and negative samples.\n",
    "        for target_product, context_product in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_product], dtype=\"int64\"), 1\n",
    "            )\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_negatives,\n",
    "                unique=True,\n",
    "                range_max=num_products,\n",
    "                seed=seed,\n",
    "                name=\"negative_sampling\",\n",
    "            )\n",
    "            \n",
    "        # Build context and label vectors (for one target product)\n",
    "        context = tf.concat([tf.squeeze(context_class, 1), negative_sampling_candidates], 0)\n",
    "        label = tf.constant([1] + [0] * num_negatives, dtype=\"int64\")\n",
    "        \n",
    "        # Append each element from the training example to global lists.\n",
    "        targets.append(target_product)\n",
    "        contexts.append(context)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return targets, contexts, labels\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15336/10161584 [00:26<4:50:09, 582.81it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1855594 is out of bounds for axis 0 with size 1855574",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m skip_gram_pairs \u001b[39m=\u001b[39m generate_training_data(\n\u001b[1;32m      2\u001b[0m     train_dataset,\n\u001b[1;32m      3\u001b[0m     seed\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m      4\u001b[0m )\n",
      "Cell \u001b[0;32mIn[15], line 31\u001b[0m, in \u001b[0;36mgenerate_training_data\u001b[0;34m(dataset, window_size, num_negatives, num_products, seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m# iterate through all sequences (sessions) in the dataset \u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m sequence \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(dataset[\u001b[39m\"\u001b[39m\u001b[39mitems_clicked\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto_list()):\n\u001b[1;32m     30\u001b[0m     \u001b[39m# Generate positive skip-gram pairs for a sequence (session).\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     positive_skip_grams, _ \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mpreprocessing\u001b[39m.\u001b[39;49msequence\u001b[39m.\u001b[39;49mskipgrams(\n\u001b[1;32m     32\u001b[0m         sequence,\n\u001b[1;32m     33\u001b[0m         vocabulary_size\u001b[39m=\u001b[39;49mnum_products,\n\u001b[1;32m     34\u001b[0m         window_size\u001b[39m=\u001b[39;49mwindow_size,\n\u001b[1;32m     35\u001b[0m         sampling_table\u001b[39m=\u001b[39;49msampling_table,\n\u001b[1;32m     36\u001b[0m         negative_samples\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     37\u001b[0m         shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     38\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Iterate over each positive skip-gram pair to produce training examples\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[39m# with positive context product and negative samples.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[39mfor\u001b[39;00m target_product, context_product \u001b[39min\u001b[39;00m positive_skip_grams:\n",
      "File \u001b[0;32m~/temp/masters-data-science/.venv/lib/python3.11/site-packages/keras/src/preprocessing/sequence.py:347\u001b[0m, in \u001b[0;36mskipgrams\u001b[0;34m(sequence, vocabulary_size, window_size, negative_samples, shuffle, categorical, sampling_table, seed)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m sampling_table \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mif\u001b[39;00m sampling_table[wi] \u001b[39m<\u001b[39m random\u001b[39m.\u001b[39mrandom():\n\u001b[1;32m    348\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    350\u001b[0m window_start \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, i \u001b[39m-\u001b[39m window_size)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1855594 is out of bounds for axis 0 with size 1855574"
     ]
    }
   ],
   "source": [
    "skip_gram_pairs = generate_training_data(\n",
    "    train_dataset,\n",
    "    seed=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
